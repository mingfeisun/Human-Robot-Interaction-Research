# hri-research

## Courses
* [Algorithmic HRI (UC Berkeley)](http://people.eecs.berkeley.edu/~anca/AHRI.html)
* [Cooperative Machines (MIT)](http://ocw.mit.edu/courses/media-arts-and-sciences/mas-965-special-topics-in-media-technology-cooperative-machines-fall-2003/lecture-notes/)
* [Human-Robot Interaction (GaTech)](https://www.cc.gatech.edu/~athomaz/classes/CS8803-HRI/Home.html)
* [Human-Robot Interaction (UW)](https://sites.google.com/site/cse599k1/calendar)
* [Manipulation Algorithms (CMU)](https://personalrobotics.ri.cmu.edu/courses/16843/)
* [Human-Robot Interaction: Algorithms & Experiments (Cornell & UT Austin)](http://hriclass.com/schedule/)
* [Robot Learning from Demonstration and Interaction (UTexas)](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F17/desc.php)

## Important links
* [Conference on Robot Learning (CoRL)](http://www.robot-learning.org/home)

## Books
* Probabilistic robotics
* Multivew Geometry in Computer Vision
* Reinforcement Learning

## Datasets

* [TV Human Interaction Dataset](http://www.robots.ox.ac.uk/~alonso/tv_human_interactions.html)
> Our Interaction Dataset consists of 300 video clips collected from over 20 different TV shows and containing 4 interactions: hand shakes, high fives, hugs and kisses, as well as clips that don't contain any of the interactions. 

* [CREATIVE-IT](http://sail.usc.edu/CreativeIT/ImprovRelease.htm)
> It contains data from 16 actors, male and female, during their affective dyadic interactions ranging from 2-10 minutes each. The database contains two types of improvised interactions: 2sentence exercises and paraphrases. 

* [ UT-Interaction dataset](http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html#Data)
> The UT-Interaction dataset contains videos of continuous executions of 6 classes of human-human interactions: shake-hands, point, hug, push, kick and punch. Ground truth labels for these interactions are provided, including time intervals and bounding boxes. There is a total of 20 video sequences whose lengths are around 1 minute. Each video contains at least one execution per interaction, providing us 8 executions of human activities per video on average. Several participants with more than 15 different clothing conditions appear in the videos. The videos are taken with the resolution of 720_480, 30fps, and the height of a person in the video is about 200 pixels.

* [Initiate interaction dataset](http://www.irc.atr.jp/sets/approach_robot/)
> We collected 63 trajectories for Intention to interact, and 67 for Other distinctive intention. All trajectories are 10 seconds long, ending either when the pedestrian reached the nearest point to the robot or at the moment when they started a conversation. The data was taken in a large hall of the shopping mall.

* [JPL First-Person Interaction dataset](http://michaelryoo.com/jpl-interaction.html)
> JPL First-Person Interaction dataset (JPL-Interaction dataset) is composed of human activity videos taken from a first-person viewpoint. The dataset particularly aims to provide first-person videos of interaction-level activities, recording how things visually look from the perspective (i.e., viewpoint) of a person/robot participating in such physical interactions. 

* [ShakeFive2](https://www.projects.science.uu.nl/shakefive/)
> A collection of Human interactions with accompanying skeleton metadata.

* [SBU-Kinect-Interaction dataset](http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/README.txt)
> This package contains the SBU-Kinect-Interaction dataset version 2.0. It comprises of RGB-D video sequences of humans performing interaction activities that are recording using the Microsoft Kinect sensor.

* [First-Person Social Interactions Dataset](http://ai.stanford.edu/~alireza/Disney/)
> This dataset contains day-long videos of 8 subjects spending their day at Disney World Resort in Orlando, Florida. The cameras are mounted on a cap worn by subjects.

* [Human Interaction Images](https://vision.cs.hacettepe.edu.tr/interaction_images/)
> In order to evaluate the proposed facial descriptors and their effect on human-human interaction recognition, we collected a new image dataset that includes ten human interaction classes. These classes are boxing-punching, dining, handshaking, highfive, hugging, kicking, kissing, partying, speech and talking. Each class contains at least 150 images, forming a total of 1972 images. When collecting the dataset, we gather images such that one of the target interaction classes is present and at least one person has a visible facial region in each image. The images for the boxing-punching, handshaking, highfive, hugging, kicking, kissing and talking classes usually include two to three people, whereas the number of people in the images for the dining, party and speech classes vary significantly.

* [VLOG Dataset](http://people.eecs.berkeley.edu/~dfouhey/2017/VLOG/)
> Lifestyle VLOGs are an immensely popular genre of video that people publicly upload to Youtube to document their lives (an archetypical example that appears in VLOG is here). The actual story is perhaps a more complicated, as described very well by Roisin Kiberd here: the overall plotline of the video is generally aspirational (out of bed at 7am, cup of black coffee, cute fluffy dog). Nonetheless, the individual components (getting out of bed, pouring a cup of coffee) are accurate.

* [Kinect-based 3D Human Interaction Dataset](http://www.lmars.whu.edu.cn/prof_web/zhuxinyan/DataSetPublish/dataset.html)
> A quality depth sensor, the Microsoft Kinect, is now in millions of homes. Yet there is no publicly accessible dataset for two-person interaction recognition based on the Kinect. We are collecting a massive Kinect-based 3D human interaction dataset.

* [Multimodal Dyadic Behavior (MMDB) dataset](http://www.cbi.gatech.edu/mmdb/)
> We introduce the Multimodal Dyadic Behavior (MMDB) dataset, a unique collection of multimodal (video, audio, and physiological) recordings of the social and communicative behavior of toddlers. The MMDB contains 160 sessions of 3-5 minute semi-structured play interaction between a trained adult examiner and a child between the age of 15 and 30 months. Our play protocol is designed to elicit social attention, back-and-forth interaction, and non-verbal communication from the child. These behaviors reflect key socio-communicative milestones which are implicated in autism spectrum disorders. The MMDB dataset supports a novel problem domain for activity recognition, which consists of the decoding of dyadic social interactions between adults and children in a developmental context.

* [HICO & HICO-DET](http://www-personal.umich.edu/~ywchao/hico/)
> We introduce two new benchmarks for classifying and detecting human-object interactions (HOI) in images: HICO (Humans Interacting with Common Objects) & HICO-DET

* [UoL 3D Social Interaction Dataset](https://lcas.lincoln.ac.uk/wp/research/data-sets-software/uol-3d-social-interaction-dataset/)
> The dataset is composed of 10 sessions. Each session provides RGB-D images and skeleton tracks of 2 long videos of different activities performed by two people. Each session is zipped in a separate file, which contains a folder that has skeleton tracks in a text format and RGB (24 bits) and depth images. Each row of the skeleton text file contains information about positions (6 DoF) of 25 joints. The work is currently submitted for RO-MAN 2017. Source code to extract skeleton data and ROS-compatible files will be provided after the RO-MAN conference. Whilst we do not require registration to download we would appreciate if you could complete the form as it enables us to help assess our impact, which is a key metric for our projectâ€™s funding. If you do register we will send you an email with the link where you will find the files.
